{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import scipy.io\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "import keras\n",
    "from keras.preprocessing import image\n",
    "from keras.callbacks import ModelCheckpoint,EarlyStopping\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten, Input, Convolution2D, ZeroPadding2D, MaxPooling2D, Activation\n",
    "from keras.layers import Conv2D, AveragePooling2D\n",
    "from keras.models import Model, Sequential\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras import metrics\n",
    "\n",
    "from keras.models import model_from_json\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get images from UTKFace at https://susanqq.github.io/UTKFace/\n",
    "## In-the-wild version was used for these models\n",
    "## Based on code found on https://github.com/serengil/tensorflow-101\n",
    "\n",
    "#os.chdir(r'C:\\Users\\user\\Downloads') # change it to the path that contains table(mat).csv\n",
    "df = pd.read_csv('utk_uncropped_all/utkCrop_no16.csv', usecols=['age', 'gender', 'race', 'date&time', 'filename'])\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>race</th>\n",
       "      <th>date&amp;time</th>\n",
       "      <th>filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>20170112213500903</td>\n",
       "      <td>100_0_0_20170112213500903.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>20170112215240346</td>\n",
       "      <td>100_0_0_20170112215240346.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>20170110183726390</td>\n",
       "      <td>100_1_0_20170110183726390.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>20170112213001988</td>\n",
       "      <td>100_1_0_20170112213001988.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>20170112213303693</td>\n",
       "      <td>100_1_0_20170112213303693.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  gender  race          date&time                       filename\n",
       "0  100     0.0     0  20170112213500903  100_0_0_20170112213500903.jpg\n",
       "1  100     0.0     0  20170112215240346  100_0_0_20170112215240346.jpg\n",
       "2  100     1.0     0  20170110183726390  100_1_0_20170110183726390.jpg\n",
       "3  100     1.0     0  20170112213001988  100_1_0_20170112213001988.jpg\n",
       "4  100     1.0     0  20170112213303693  100_1_0_20170112213303693.jpg"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns = 'date&time')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#some guys seem to be greater than 100. some of these are paintings. remove these old guys\n",
    "df = df[df['age'] <= 100]\n",
    "\n",
    "#some guys seem to be unborn in the data set\n",
    "df = df[df['age'] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>race</th>\n",
       "      <th>filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>100_0_0_20170112213500903.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>100_0_0_20170112215240346.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>100_1_0_20170110183726390.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>100_1_0_20170112213001988.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>100_1_0_20170112213303693.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  gender  race                       filename\n",
       "0  100     0.0     0  100_0_0_20170112213500903.jpg\n",
       "1  100     0.0     0  100_0_0_20170112215240346.jpg\n",
       "2  100     1.0     0  100_1_0_20170110183726390.jpg\n",
       "3  100     1.0     0  100_1_0_20170112213001988.jpg\n",
       "4  100     1.0     0  100_1_0_20170112213303693.jpg"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAD4CAYAAAAO9oqkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAARfUlEQVR4nO3dfZBddX3H8fdHIorxATCywyTU4BitSKYj3QGsM3ZrHAjYIfwBnTgqgUmbGYvW2kxbbP9IB2RG21IqjNqmkhIcKiB1moxiaQbYse0YBMTyWIYUKKxQ0SakRurD2m//uL+0K2z24d7du9nd92tmZ8/5nd859/vNbu4n59xzb1JVSJIWt5fMdQGSpLlnGEiSDANJkmEgScIwkCQBS+a6gG4tW7asVq5c2dW+P/jBD1i6dOnMFnSYs+eFb7H1C/Y8Xffcc8/3qup1422bt2GwcuVK7r777q72HR4eZmhoaGYLOszZ88K32PoFe56uJP9+qG1eJpIkGQaSJMNAkoRhIEnCMJAkYRhIkjAMJEkYBpIkDANJEvP4Hci9uP/b+7nwkq/MdRl9tXn1qD33yROfeE/fH1PqlWcGkiTDQJJkGEiSMAwkSRgGkiQMA0kSUwiDJNuSPJvkgTFjxybZleTR9v2YNp4kVyXZk+S+JKeM2WdDm/9okg1jxn8xyf1tn6uSZKablCRNbCpnBtcCa18wdglwW1WtAm5r6wBnAava1ybgs9AJD2ALcBpwKrDlYIC0OZvG7PfCx5IkzbJJw6CqvgbsfcHwOmB7W94OnDtm/Lrq2A0cneR44ExgV1Xtrap9wC5gbdv26qr6elUVcN2YY0mS+qTbdyAPVNUzAFX1TJLj2vhy4Kkx80ba2ETjI+OMjyvJJjpnEQwMDDA8PNxd8Ud13p26mNhz/3T7e9mrAwcOzNljzxV7njkz/XEU413vry7Gx1VVW4GtAIODg9Xtfwp99fU7uOL+xfVJHJtXj9pznzzxvqG+Pyb4n8MvFrPVc7d3E32nXeKhfX+2jY8AJ4yZtwJ4epLxFeOMS5L6qNsw2AkcvCNoA7BjzPgF7a6i04H97XLSrcAZSY5pLxyfAdzatn0/yentLqILxhxLktQnk55DJ/kCMAQsSzJC566gTwA3JdkIPAmc36bfApwN7AGeBy4CqKq9SS4D7mrzLq2qgy9Kf5DOHUtHAV9tX5KkPpo0DKrqvYfYtGacuQVcfIjjbAO2jTN+N3DyZHVIkmaP70CWJBkGkiTDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJAwDSRKGgSSJmf8Ia2nRW3nJV+bkcTevHuXCOXrsubIYe7527dJZOa5nBpIkw0CSZBhIkjAMJEkYBpIkDANJEoaBJAnDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJAwDSRKGgSQJw0CShGEgScIwkCRhGEiS6DEMknw0yYNJHkjyhSQvT3JikjuTPJrkxiRHtrkva+t72vaVY47zsTb+SJIze2tJkjRdXYdBkuXAbwGDVXUycASwHvgkcGVVrQL2ARvbLhuBfVX1RuDKNo8kJ7X93gqsBT6T5Ihu65IkTV+vl4mWAEclWQK8AngGeBdwc9u+HTi3La9r67Tta5Kkjd9QVT+qqseBPcCpPdYlSZqGJd3uWFXfTvKnwJPAfwP/ANwDPFdVo23aCLC8LS8Hnmr7jibZD7y2je8ec+ix+/yMJJuATQADAwMMDw93VfvAUbB59ejkExcQe174Flu/sDh7PnDgQNfPfRPpOgySHEPnX/UnAs8BXwTOGmdqHdzlENsONf7iwaqtwFaAwcHBGhoaml7RzdXX7+CK+7tufV7avHrUnhe4xdYvLM6er127lG6f+ybSy2WidwOPV9V3q+onwJeAXwKObpeNAFYAT7flEeAEgLb9NcDesePj7CNJ6oNewuBJ4PQkr2jX/tcADwF3AOe1ORuAHW15Z1unbb+9qqqNr293G50IrAK+0UNdkqRp6uU1gzuT3Ax8ExgF7qVzCecrwA1JPt7Grmm7XAN8PskeOmcE69txHkxyE50gGQUurqqfdluXJGn6errYVlVbgC0vGH6Mce4GqqofAucf4jiXA5f3UoskqXu+A1mSZBhIkgwDSRKGgSQJw0CShGEgScIwkCRhGEiSMAwkSRgGkiQMA0kShoEkCcNAkoRhIEnCMJAkYRhIkjAMJEkYBpIkDANJEoaBJAnDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJAwDSRKGgSQJw0CShGEgSaLHMEhydJKbk/xrkoeTvD3JsUl2JXm0fT+mzU2Sq5LsSXJfklPGHGdDm/9okg29NiVJmp5ezww+Bfx9Vf088AvAw8AlwG1VtQq4ra0DnAWsal+bgM8CJDkW2AKcBpwKbDkYIJKk/ug6DJK8GngncA1AVf24qp4D1gHb27TtwLlteR1wXXXsBo5OcjxwJrCrqvZW1T5gF7C227okSdPXy5nBG4DvAn+d5N4kn0uyFBioqmcA2vfj2vzlwFNj9h9pY4calyT1yZIe9z0F+HBV3ZnkU/z/JaHxZJyxmmD8xQdINtG5xMTAwADDw8PTKviggaNg8+rRrvadr+x54Vts/cLi7PnAgQNdP/dNpJcwGAFGqurOtn4znTD4TpLjq+qZdhno2THzTxiz/wrg6TY+9ILx4fEesKq2AlsBBgcHa2hoaLxpk7r6+h1ccX8vrc8/m1eP2vMCt9j6hcXZ87Vrl9Ltc99Eur5MVFX/ATyV5M1taA3wELATOHhH0AZgR1veCVzQ7io6HdjfLiPdCpyR5Jj2wvEZbUyS1Ce9RuqHgeuTHAk8BlxEJ2BuSrIReBI4v829BTgb2AM83+ZSVXuTXAbc1eZdWlV7e6xLkjQNPYVBVX0LGBxn05px5hZw8SGOsw3Y1kstkqTu+Q5kSZJhIEkyDCRJGAaSJAwDSRKGgSQJw0CShGEgScIwkCRhGEiSMAwkSRgGkiQMA0kShoEkCcNAkoRhIEnCMJAkYRhIkjAMJEkYBpIkDANJEoaBJAnDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJAwDSRKGgSQJw0CSxAyEQZIjktyb5Mtt/cQkdyZ5NMmNSY5s4y9r63va9pVjjvGxNv5IkjN7rUmSND0zcWbwEeDhMeufBK6sqlXAPmBjG98I7KuqNwJXtnkkOQlYD7wVWAt8JskRM1CXJGmKegqDJCuA9wCfa+sB3gXc3KZsB85ty+vaOm37mjZ/HXBDVf2oqh4H9gCn9lKXJGl6lvS4/58Dvwe8qq2/Fniuqkbb+giwvC0vB54CqKrRJPvb/OXA7jHHHLvPz0iyCdgEMDAwwPDwcFdFDxwFm1ePTj5xAbHnhW+x9QuLs+cDBw50/dw3ka7DIMmvAs9W1T1Jhg4OjzO1Jtk20T4/O1i1FdgKMDg4WENDQ+NNm9TV1+/givt7zcH5ZfPqUXte4BZbv7A4e7527VK6fe6bSC9/iu8AzklyNvBy4NV0zhSOTrKknR2sAJ5u80eAE4CRJEuA1wB7x4wfNHYfSVIfdP2aQVV9rKpWVNVKOi8A315V7wPuAM5r0zYAO9ryzrZO2357VVUbX9/uNjoRWAV8o9u6JEnTNxvnV78P3JDk48C9wDVt/Brg80n20DkjWA9QVQ8muQl4CBgFLq6qn85CXZKkQ5iRMKiqYWC4LT/GOHcDVdUPgfMPsf/lwOUzUYskafp8B7IkyTCQJBkGkiQMA0kShoEkCcNAkoRhIEnCMJAkYRhIkjAMJEkYBpIkDANJEoaBJAnDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJAwDSRKGgSQJw0CShGEgScIwkCRhGEiSMAwkSRgGkiQMA0kShoEkCcNAkkQPYZDkhCR3JHk4yYNJPtLGj02yK8mj7fsxbTxJrkqyJ8l9SU4Zc6wNbf6jSTb03pYkaTp6OTMYBTZX1VuA04GLk5wEXALcVlWrgNvaOsBZwKr2tQn4LHTCA9gCnAacCmw5GCCSpP7oOgyq6pmq+mZb/j7wMLAcWAdsb9O2A+e25XXAddWxGzg6yfHAmcCuqtpbVfuAXcDabuuSJE3fkpk4SJKVwNuAO4GBqnoGOoGR5Lg2bTnw1JjdRtrYocbHe5xNdM4qGBgYYHh4uKt6B46CzatHu9p3vrLnhW+x9QuLs+cDBw50/dw3kZ7DIMkrgb8Ffruq/ivJIaeOM1YTjL94sGorsBVgcHCwhoaGpl0vwNXX7+CK+2ckB+eNzatH7XmBW2z9wuLs+dq1S+n2uW8iPd1NlOSldILg+qr6Uhv+Trv8Q/v+bBsfAU4Ys/sK4OkJxiVJfdLL3UQBrgEerqo/G7NpJ3DwjqANwI4x4xe0u4pOB/a3y0m3AmckOaa9cHxGG5Mk9Ukv51fvAD4A3J/kW23sD4BPADcl2Qg8CZzftt0CnA3sAZ4HLgKoqr1JLgPuavMuraq9PdQlSZqmrsOgqv6J8a/3A6wZZ34BFx/iWNuAbd3WIknqje9AliQZBpIkw0CShGEgScIwkCRhGEiSMAwkSRgGkiQMA0kShoEkCcNAkoRhIEnCMJAkYRhIkjAMJEkYBpIkDANJEoaBJAnDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJAwDSRKGgSQJw0CShGEgScIwkCRhGEiSMAwkSRxGYZBkbZJHkuxJcslc1yNJi8lhEQZJjgA+DZwFnAS8N8lJc1uVJC0eh0UYAKcCe6rqsar6MXADsG6Oa5KkRSNVNdc1kOQ8YG1V/Xpb/wBwWlV96AXzNgGb2uqbgUe6fMhlwPe63He+sueFb7H1C/Y8Xa+vqteNt2FJ9/XMqIwz9qKUqqqtwNaeHyy5u6oGez3OfGLPC99i6xfseSYdLpeJRoATxqyvAJ6eo1okadE5XMLgLmBVkhOTHAmsB3bOcU2StGgcFpeJqmo0yYeAW4EjgG1V9eAsPmTPl5rmIXte+BZbv2DPM+aweAFZkjS3DpfLRJKkOWQYSJIWdhhM9hEXSV6W5Ma2/c4kK/tf5cyZQr+/k+ShJPcluS3J6+eizpk01Y8xSXJekkoy729DnErPSX6t/awfTPI3/a5xpk3hd/vnktyR5N72+332XNQ5U5JsS/JskgcOsT1Jrmp/HvclOaXnB62qBflF54XofwPeABwJ/Atw0gvm/CbwF215PXDjXNc9y/3+CvCKtvzB+dzvVHtu814FfA3YDQzOdd19+DmvAu4Fjmnrx8113X3oeSvwwbZ8EvDEXNfdY8/vBE4BHjjE9rOBr9J5j9bpwJ29PuZCPjOYykdcrAO2t+WbgTVJxnsD3Hwwab9VdUdVPd9Wd9N5P8d8NtWPMbkM+GPgh/0sbpZMpeffAD5dVfsAqurZPtc406bScwGvbsuvYZ6/T6mqvgbsnWDKOuC66tgNHJ3k+F4ecyGHwXLgqTHrI21s3DlVNQrsB17bl+pm3lT6HWsjnX9ZzGeT9pzkbcAJVfXlfhY2i6byc34T8KYk/5xkd5K1fatudkyl5z8C3p9kBLgF+HB/Spsz0/37PqnD4n0Gs2QqH3ExpY/BmCem3EuS9wODwC/PakWzb8Kek7wEuBK4sF8F9cFUfs5L6FwqGqJz9vePSU6uqudmubbZMpWe3wtcW1VXJHk78PnW8//MfnlzYsafuxbymcFUPuLi/+YkWULn9HKiU7PD2ZQ+0iPJu4E/BM6pqh/1qbbZMlnPrwJOBoaTPEHn2urOef4i8lR/r3dU1U+q6nE6H+i4qk/1zYap9LwRuAmgqr4OvJzOB7otVDP+ET4LOQym8hEXO4ENbfk84PZqr87MQ5P22y6Z/CWdIJjv15Fhkp6ran9VLauqlVW1ks7rJOdU1d1zU+6MmMrv9d/RuVmAJMvoXDZ6rK9Vzqyp9PwksAYgyVvohMF3+1plf+0ELmh3FZ0O7K+qZ3o54IK9TFSH+IiLJJcCd1fVTuAaOqeTe+icEayfu4p7M8V+/wR4JfDF9jr5k1V1zpwV3aMp9rygTLHnW4EzkjwE/BT43ar6z7mrujdT7Hkz8FdJPkrncsmF8/gfdiT5Ap3LfMva6yBbgJcCVNVf0Hld5GxgD/A8cFHPjzmP/7wkSTNkIV8mkiRNkWEgSTIMJEmGgSQJw0CShGEgScIwkCQB/wvxo3SpyfoMdgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "histogram = df['gender'].hist(bins=df['gender'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    10517\n",
       "1.0     9197\n",
       "Name: gender, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['gender'].value_counts().sort_index()\n",
    "#0: woman, 1: man"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of output classes:  2\n"
     ]
    }
   ],
   "source": [
    "classes = 2 #man woman\n",
    "print(\"number of output classes: \",classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_size = (224, 224)\n",
    "\n",
    "def getImagePixels(image_path):\n",
    "    img = image.load_img(\"utk_uncropped_all/%s\" % image_path, grayscale=False, target_size=target_size)\n",
    "    x = image.img_to_array(img).reshape(1, -1)[0]\n",
    "    #x = preprocess_input(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['pixels'] = df['filename'].apply(getImagePixels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>race</th>\n",
       "      <th>filename</th>\n",
       "      <th>pixels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>100_0_0_20170112213500903.jpg</td>\n",
       "      <td>[51.0, 55.0, 64.0, 82.0, 86.0, 95.0, 73.0, 77....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>100_0_0_20170112215240346.jpg</td>\n",
       "      <td>[113.0, 117.0, 126.0, 113.0, 117.0, 126.0, 114...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>100_1_0_20170110183726390.jpg</td>\n",
       "      <td>[226.0, 229.0, 236.0, 232.0, 235.0, 242.0, 227...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>100_1_0_20170112213001988.jpg</td>\n",
       "      <td>[65.0, 72.0, 80.0, 65.0, 72.0, 80.0, 65.0, 72....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>100_1_0_20170112213303693.jpg</td>\n",
       "      <td>[31.0, 36.0, 40.0, 31.0, 36.0, 40.0, 31.0, 36....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  gender  race                       filename  \\\n",
       "0  100     0.0     0  100_0_0_20170112213500903.jpg   \n",
       "1  100     0.0     0  100_0_0_20170112215240346.jpg   \n",
       "2  100     1.0     0  100_1_0_20170110183726390.jpg   \n",
       "3  100     1.0     0  100_1_0_20170112213001988.jpg   \n",
       "4  100     1.0     0  100_1_0_20170112213303693.jpg   \n",
       "\n",
       "                                              pixels  \n",
       "0  [51.0, 55.0, 64.0, 82.0, 86.0, 95.0, 73.0, 77....  \n",
       "1  [113.0, 117.0, 126.0, 113.0, 117.0, 126.0, 114...  \n",
       "2  [226.0, 229.0, 236.0, 232.0, 235.0, 242.0, 227...  \n",
       "3  [65.0, 72.0, 80.0, 65.0, 72.0, 80.0, 65.0, 72....  \n",
       "4  [31.0, 36.0, 40.0, 31.0, 36.0, 40.0, 31.0, 36....  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = df['gender'].values\n",
    "target_classes = keras.utils.to_categorical(target, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#features = df['pixels'].values\n",
    "features = []\n",
    "\n",
    "for i in range(0, df.shape[0]):\n",
    "    features.append(df['pixels'].values[i])\n",
    "\n",
    "features = np.array(features)\n",
    "features = features.reshape(features.shape[0], 224, 224, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19714, 224, 224, 3)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "features /= 255 #normalize in [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, test_x, train_y, test_y = train_test_split(features, target_classes, test_size=0.30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VGG-Face model\n",
    "model = Sequential()\n",
    "model.add(ZeroPadding2D((1,1),input_shape=(224,224, 3)))\n",
    "model.add(Convolution2D(64, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    " \n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(128, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    " \n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(256, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(256, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(256, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    " \n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    " \n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    " \n",
    "model.add(Convolution2D(4096, (7, 7), activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Convolution2D(4096, (1, 1), activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Convolution2D(2622, (1, 1)))\n",
    "model.add(Flatten())\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pre-trained weights of vgg-face model. \n",
    "#you can find it here: https://drive.google.com/file/d/1CPSeum3HpopfomUEK1gybeuIVoeJT_Eo/view?usp=sharing\n",
    "#related blog post: https://sefiks.com/2018/08/06/deep-face-recognition-with-keras/\n",
    "model.load_weights('vgg_face_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#freeze all layers of VGG-Face except last 7 one\n",
    "for layer in model.layers[:-7]:\n",
    "    layer.trainable = False\n",
    "\n",
    "base_model_output = Sequential()\n",
    "base_model_output = Convolution2D(classes, (1, 1), name='predictions')(model.layers[-4].output)\n",
    "base_model_output = Flatten()(base_model_output)\n",
    "base_model_output = Activation('softmax')(base_model_output)\n",
    "\n",
    "gender_model = Model(inputs=model.input, outputs=base_model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check trainable layers\n",
    "#if False:\n",
    "for layer in model.layers:\n",
    "    print(layer, layer.trainable)\n",
    "\n",
    "print(\"------------------------\")\n",
    "for layer in gender_model.layers:\n",
    "    print(layer, layer.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = keras.optimizers.SGD(lr=1e-3, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "\n",
    "gender_model.compile(loss='categorical_crossentropy'\n",
    "                  , optimizer=keras.optimizers.Adam()\n",
    "                  #, optimizer = sgd\n",
    "                  , metrics=['accuracy']\n",
    "\n",
    "                     , options=run_opts\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointer = ModelCheckpoint(\n",
    "    filepath='classification_gender_model.hdf5'\n",
    "    , monitor = \"val_loss\"\n",
    "    , verbose=1\n",
    "    , save_best_only=True\n",
    "    , mode = 'auto'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "enableFit = False\n",
    "\n",
    "if enableFit:\n",
    "    epochs = 250\n",
    "    batch_size = 512\n",
    "\n",
    "    for i in range(epochs):\n",
    "        print(\"epoch \",i)\n",
    "        \n",
    "        ix_train = np.random.choice(train_x.shape[0], size=batch_size)\n",
    "        \n",
    "        score = gender_model.fit(\n",
    "            train_x[ix_train], train_y[ix_train]\n",
    "            , epochs=1\n",
    "            , validation_data=(test_x, test_y)\n",
    "            , callbacks=[checkpointer]\n",
    "        )\n",
    "        \n",
    "        scores.append(score)\n",
    "        \n",
    "        from keras.models import load_model\n",
    "        gender_model = load_model(\"classification_gender_model.hdf5\")\n",
    "        \n",
    "        gender_model.save_weights('gender_model_weights_utk.h5')\n",
    "        \n",
    "else:\n",
    "    #pre-trained weights for gender prediction: https://drive.google.com/file/d/1wUXRVlbsni2FN9-jkS_f4UTUrm1bRLyk/view?usp=sharing\n",
    "    gender_model.load_weights(\"gender_model_weights_utk.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss_change = []; loss_change = []\n",
    "for i in range(0, len(scores)):\n",
    "    val_loss_change.append(scores[i].history['val_loss'])\n",
    "    loss_change.append(scores[i].history['loss'])\n",
    "\n",
    "plt.plot(val_loss_change, label='val_loss')\n",
    "plt.plot(loss_change, label='train_loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing model on the testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss and accuracy on validation set\n",
    "gender_model.evaluate(test_x, test_y, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = gender_model.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "pred_list = []; actual_list = []\n",
    "\n",
    "for i in predictions:\n",
    "    pred_list.append(np.argmax(i))\n",
    "\n",
    "for i in test_y: \n",
    "    actual_list.append(np.argmax(i))\n",
    "\n",
    "confusion_matrix(actual_list, pred_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing model\n",
    "\n",
    "Feed an image to find the gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadImage(filepath):\n",
    "    test_img = image.load_img(filepath, target_size=(224, 224))\n",
    "    test_img = image.img_to_array(test_img)\n",
    "    test_img = np.expand_dims(test_img, axis = 0)\n",
    "    test_img /= 255\n",
    "    return test_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "picture = \"Robbie.jpg\" #replace by desired image to predict gender on\n",
    "\n",
    "prediction = gender_model.predict(loadImage(picture))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = image.load_img(picture)#, target_size=(224, 224))\n",
    "plt.imshow(img)\n",
    "plt.show()\n",
    "\n",
    "gender = \"Man\" if np.argmax(prediction) == 1 else \"Woman\"\n",
    "\n",
    "print(\"gender: \", gender)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test on UTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "utk_info = pd.read_csv('utkCrop.csv', usecols=['age', 'gender', 'race', 'date&time', 'filename'])\n",
    "utk_info.dropna(inplace=True)\n",
    "pred_gender = []         \n",
    "#os.chdir(r'C:\\Users\\user\\Downloads\\') # change it to where the corresponding pics locates\n",
    "count=0\n",
    "for file in utk_info['filename']:\n",
    "    count+=1\n",
    "    if count%1000 ==0:\n",
    "        print(count)\n",
    "    prediction = gender_model.predict(loadImage(file))\n",
    "    pred_gender = np.argmax(prediction)\n",
    "utk_info['pred_gender'] = pred_gender\n",
    "utk_info;"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
